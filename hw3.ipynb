{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "velvet-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: DeRafael\n",
    "In this part we construct the fully connect neural network models on  MNIST\n",
    "\"\"\"\n",
    "# import packages\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "editorial-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter setting\n",
    "training_epochs = 30\n",
    "step_size = 1e-2\n",
    "batch_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aquatic-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# MNIST 60000 training data & 10000 testing data  x: (?, 28, 28); y (?,)\n",
    "# data are stored as numpy format\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "Feature_dimension = 784\n",
    "N_train = x_train.shape[0] # 60000\n",
    "N_test = x_test.shape[0] # 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "simple-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build training dataset and testing dataset\n",
    "# We reshape the training data to [?, 784] and convert them to tensor\n",
    "# .batch() sets the batch size and shuffle the data\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((tf.reshape(x_train, [-1, Feature_dimension]), y_train))\n",
    "    .batch(batch_size).shuffle(buffer_size=N_train, seed=0)\n",
    ")\n",
    "\n",
    "# we preprocess the data: divide the images by 255 and cast the data format to tf.float32 other than tf.int\n",
    "# onehot will change the label from a number to a one hot vector\n",
    "# e.g. 6 -> [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "train_dataset = (\n",
    "    train_dataset.map(lambda x, y:\n",
    "                      (tf.divide(tf.cast(x, tf.float32), 255.0),\n",
    "                       tf.reshape(tf.one_hot(y, 10), (-1, 10))))\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((tf.reshape(x_test, [-1, Feature_dimension]), y_test)).batch(N_test)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    test_dataset.map(lambda x, y:\n",
    "                      (tf.divide(tf.cast(x, tf.float32), 255.0),\n",
    "                       tf.reshape(tf.one_hot(y, 10), (-1, 10))))\n",
    ")\n",
    "\n",
    "# the data type of the two datasets are MapDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "neither-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural networks\n",
    "# we build a class\n",
    "class neural_netowrk(tf.keras.Model):\n",
    "    def __init__(self, seed=1):\n",
    "        super(neural_netowrk, self).__init__()\n",
    "        # use random seed to make the initialization repeat\n",
    "        tf.random.set_seed(seed)\n",
    "        # define fully connected layers\n",
    "        # function: tf.keras.layers.Dense(number_of_output_nodes, activation=activation_functions, name=layer_name)\n",
    "        # the input tensor is batch_size x feature(784)\n",
    "        self.fc1 = tf.keras.layers.Dense(100, activation = 'sigmoid', name='fc1')\n",
    "        # the output of the first layer is batch_size x 100\n",
    "        self.fc2 = tf.keras.layers.Dense(10, activation='softmax', name='fc2')\n",
    "        # the output of the second layer is batch_size x 10 (we have already include the softmax function)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        here we define the forward function\n",
    "        :param input: the input data\n",
    "        :return: output tensor\n",
    "        '''\n",
    "        # For each layer, a bias will also be initialized and add to the output after matrix multiply.\n",
    "        x = self.fc1(input)\n",
    "        output = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "handy-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.losses.binary_crossentropy is the cross entropy function function\n",
    "def compute_loss(true, pred):\n",
    "    '''\n",
    "    :param true: true labels\n",
    "    :param pred: output tensor\n",
    "    :return: loss\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.keras.metrics.categorical_crossentropy(true, pred), axis=-1))  # cross entropy\n",
    "\n",
    "# compute accuracy\n",
    "# we use the function tf.keras.metrics.categorical_accuracy() to compute the accuracy\n",
    "def compute_accuracy(true, pred):\n",
    "    '''\n",
    "    :param true: true labels\n",
    "    :param pred: output tensor\n",
    "    :return: accuracy\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.keras.metrics.categorical_accuracy(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acceptable-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function:\n",
    "def check(model):\n",
    "    '''\n",
    "    :param model: the neural network\n",
    "    :return: training loss; training accuracy; testing loss; testing accuracy\n",
    "    '''\n",
    "    loss = []\n",
    "    acc = []\n",
    "    for x, y in train_dataset:\n",
    "        output = model.forward(x)\n",
    "        loss.append(compute_loss(y, output))\n",
    "        acc.append(compute_accuracy(y, output))\n",
    "    train_loss = sum(loss) / len(loss)\n",
    "    train_acc = sum(acc) / len(acc)\n",
    "    # we can use zip function to seperate data and labels\n",
    "    test_x, test_y = zip(*test_dataset)\n",
    "    test_x = test_x[0]\n",
    "    test_y = test_y[0]\n",
    "\n",
    "    output = model.forward(test_x)\n",
    "    test_loss = compute_loss(test_y, output)\n",
    "    test_acc = compute_accuracy(test_y, output)\n",
    "    # tesnor.numpy() can convert the tensor to numpy format\n",
    "    return train_loss.numpy(), train_acc.numpy(), test_loss.numpy(), test_acc.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "assigned-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    epoch = 0\n",
    "    # define the model\n",
    "    model = neural_netowrk()\n",
    "    # define the optimizer\n",
    "    # here we use the adam optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=step_size)\n",
    "    # set the loop conditions\n",
    "    while epoch < training_epochs:\n",
    "        epoch += 1\n",
    "        # training\n",
    "        for x, y in train_dataset: # go through all the training data\n",
    "            # we come to the most important part, using tf.GradientTape\n",
    "            # tensorflow will compute the gradient of the parameter by tf.GradientTape\n",
    "            with tf.GradientTape() as tape:\n",
    "                # go forward we get the output\n",
    "                output = model.forward(x)\n",
    "                # compute training loss\n",
    "                loss = compute_loss(y, output)\n",
    "            # ask for the gradient\n",
    "            # tape.gradient(target=loss, source=parameter)\n",
    "            grads = tape.gradient(target=loss, sources=model.trainable_variables) # the type of the gradients is list\n",
    "            # optimize parameter\n",
    "            # zip function is needed to align the gradients and parameters\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        # every 5 epochs, we compute the training loss on the whole training data and the testing loss\n",
    "        if epoch % 5 == 1:\n",
    "            train_loss, train_acc, test_loss, test_acc = check(model)\n",
    "            # print the results\n",
    "            print('training loss: %.3f, training accuracy: %.3f, testing loss: %.3f, testing accuracy: %.3f'\n",
    "                  %(train_loss/batch_size, float(train_acc), test_loss / N_test, float(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alternative-liechtenstein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.330, training accuracy: 0.904, testing loss: 0.316, testing accuracy: 0.910\n",
      "training loss: 0.148, training accuracy: 0.955, testing loss: 0.160, testing accuracy: 0.950\n",
      "training loss: 0.095, training accuracy: 0.973, testing loss: 0.120, testing accuracy: 0.963\n",
      "training loss: 0.084, training accuracy: 0.975, testing loss: 0.121, testing accuracy: 0.964\n",
      "training loss: 0.064, training accuracy: 0.983, testing loss: 0.111, testing accuracy: 0.967\n",
      "training loss: 0.056, training accuracy: 0.985, testing loss: 0.107, testing accuracy: 0.969\n"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-junior",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
